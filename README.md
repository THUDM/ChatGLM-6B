# ChatGLM-6B

## ä»‹ç»

ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº [General Language Model (GLM)](https://github.com/THUDM/GLM) æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚ç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼ˆINT4 é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€ 6GB æ˜¾å­˜ï¼‰ã€‚ChatGLM-6B ä½¿ç”¨äº†å’Œ ChatGPT ç›¸ä¼¼çš„æŠ€æœ¯ï¼Œé’ˆå¯¹ä¸­æ–‡é—®ç­”å’Œå¯¹è¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç»è¿‡çº¦ 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥ç›‘ç£å¾®è°ƒã€åé¦ˆè‡ªåŠ©ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯çš„åŠ æŒï¼Œ62 äº¿å‚æ•°çš„ ChatGLM-6B å·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è€ƒæˆ‘ä»¬çš„[åšå®¢](https://chatglm.cn/blog)ã€‚

åŒæ—¶ï¼Œæˆ‘ä»¬åŸºäºåƒäº¿åŸºåº§çš„[ChatGLM æ¨¡å‹](https://chatglm.cn)æ­£åœ¨é‚€è¯·åˆ¶å†…æµ‹ï¼Œåç»­å°†é€æ­¥æ‰©å¤§å†…æµ‹èŒƒå›´ï¼Œæ¬¢è¿ç”³è¯·åŠ å…¥å†…æµ‹ã€‚

## ç¡¬ä»¶éœ€æ±‚

| **é‡åŒ–ç­‰çº§**    | **æœ€ä½ GPU æ˜¾å­˜** |
| -------------- | ----------------- |
| FP16ï¼ˆæ— é‡åŒ–ï¼‰   | 13 GB             |
| INT8           | 10 GB              |
| INT4           | 6 GB               |

## ä½¿ç”¨æ–¹å¼

### ç¯å¢ƒå®‰è£…

ä½¿ç”¨ pip å®‰è£…ä¾èµ–ï¼š`pip install -r requirements.txt`ï¼Œå…¶ä¸­ `transformers` åº“ç‰ˆæœ¬æ¨èä¸º `4.26.1`ï¼Œä½†ç†è®ºä¸Šä¸ä½äº `4.23.1` å³å¯ã€‚

### ä»£ç è°ƒç”¨ 

å¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç è°ƒç”¨ ChatGLM-6B æ¨¡å‹æ¥ç”Ÿæˆå¯¹è¯ï¼š

```python
>>> from transformers import AutoTokenizer, AutoModel
>>> tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
>>> model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
>>> response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
>>> print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
>>> response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
>>> print(response)
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:

1. åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚
2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£ã€‚
3. æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚
4. é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹ã€‚
5. é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,ç©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚
6. å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åç¼“æ…¢å‘¼æ°”ã€‚

å¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡,ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶,å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®ã€‚
```
å®Œæ•´çš„æ¨¡å‹å®ç°å¯ä»¥åœ¨ [HuggingFace Hub](https://huggingface.co/THUDM/chatglm-6b) ä¸ŠæŸ¥çœ‹ã€‚

### Demo

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŸºäº [Gradio](https://gradio.app) çš„ç½‘é¡µç‰ˆ Demo å’Œä¸€ä¸ªå‘½ä»¤è¡Œ Demoã€‚ä½¿ç”¨æ—¶é¦–å…ˆéœ€è¦ä¸‹è½½æœ¬ä»“åº“ï¼š

```shell
git clone https://github.com/THUDM/ChatGLM-6B
cd ChatGLM-6B
```

#### ç½‘é¡µç‰ˆ Demo

![web-demo](resources/web-demo.png)

é¦–å…ˆå®‰è£… Gradioï¼š`pip install gradio`ï¼Œç„¶åè¿è¡Œä»“åº“ä¸­çš„ [web_demo.py](web_demo.py)ï¼š 

```shell
python web_demo.py
```

ç¨‹åºä¼šè¿è¡Œä¸€ä¸ª Web Serverï¼Œå¹¶è¾“å‡ºåœ°å€ã€‚åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¾“å‡ºçš„åœ°å€å³å¯ä½¿ç”¨ã€‚

#### å‘½ä»¤è¡Œ Demo

![cli-demo](resources/cli-demo.png)

è¿è¡Œä»“åº“ä¸­ [cli_demo.py](cli_demo.py)ï¼š

```shell
python cli_demo.py
```

ç¨‹åºä¼šåœ¨å‘½ä»¤è¡Œä¸­è¿›è¡Œäº¤äº’å¼çš„å¯¹è¯ï¼Œåœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥æŒ‡ç¤ºå¹¶å›è½¦å³å¯ç”Ÿæˆå›å¤ï¼Œè¾“å…¥`clear`å¯ä»¥æ¸…ç©ºå¯¹è¯å†å²ï¼Œè¾“å…¥`stop`ç»ˆæ­¢ç¨‹åºã€‚

## æ¨¡å‹é‡åŒ–
é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä»¥ FP16 ç²¾åº¦åŠ è½½ï¼Œè¿è¡Œä¸Šè¿°ä»£ç éœ€è¦å¤§æ¦‚ 13GB æ˜¾å­˜ã€‚å¦‚æœä½ çš„ GPU æ˜¾å­˜æœ‰é™ï¼Œå¯ä»¥å°è¯•ä»¥é‡åŒ–æ–¹å¼åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š

```python
# æŒ‰éœ€ä¿®æ”¹ï¼Œç›®å‰åªæ”¯æŒ 4/8 bit é‡åŒ–
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().quantize(4).cuda()
```

è¿›è¡Œ 2 è‡³ 3 è½®å¯¹è¯åï¼Œ8-bit é‡åŒ–ä¸‹ GPU æ˜¾å­˜å ç”¨çº¦ä¸º 10GBï¼Œ4-bit é‡åŒ–ä¸‹ä»…éœ€ 6GB å ç”¨ã€‚éšç€å¯¹è¯è½®æ•°çš„å¢å¤šï¼Œå¯¹åº”æ¶ˆè€—æ˜¾å­˜ä¹Ÿéšä¹‹å¢é•¿ï¼Œç”±äºé‡‡ç”¨äº†ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œç†è®ºä¸Š ChatGLM-6B æ”¯æŒæ— é™é•¿çš„ context-lengthï¼Œä½†æ€»é•¿åº¦è¶…è¿‡ 2048ï¼ˆè®­ç»ƒé•¿åº¦ï¼‰åæ€§èƒ½ä¼šé€æ¸ä¸‹é™ã€‚

æ¨¡å‹é‡åŒ–ä¼šå¸¦æ¥ä¸€å®šçš„æ€§èƒ½æŸå¤±ï¼Œç»è¿‡æµ‹è¯•ï¼ŒChatGLM-6B åœ¨ 4-bit é‡åŒ–ä¸‹ä»ç„¶èƒ½å¤Ÿè¿›è¡Œè‡ªç„¶æµç•…çš„ç”Ÿæˆï¼Œä½¿ç”¨ [GPT-Q](https://arxiv.org/abs/2210.17323) ç­‰é‡åŒ–æ–¹æ¡ˆå¯ä»¥è¿›ä¸€æ­¥å‹ç¼©é‡åŒ–ç²¾åº¦/æå‡ç›¸åŒé‡åŒ–ç²¾åº¦ä¸‹çš„æ¨¡å‹æ€§èƒ½ï¼Œæˆ‘ä»¬æœŸå¾…å¼€æºç¤¾åŒºä¸ºæœ¬é¡¹ç›®æä¾›å¯¹åº” Pull Requestã€‚

## ChatGLM-6Bç¤ºä¾‹

<details><summary><b>ç‚¹å‡»å±•å¼€</b></summary>

![](examples/self-introduction.png)

![](examples/blog-outline.png)

![](examples/ad-writing.png)

![](examples/ad-writing-2.png)

![](examples/comments-writing.png)

![](examples/email-writing-1.png)

![](examples/email-writing-2.png)

![](examples/information-extraction.png)

![](examples/role-play.png)

![](examples/sport.png)

![](examples/tour-guide.png)

</details>

## åè®®

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ [Apache-2.0](LICENSE) åè®®å¼€æºï¼ŒChatGLM-6B æ¨¡å‹çš„æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª [Model License](MODEL_LICENSE)ã€‚

## å¼•ç”¨

å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä¸‹åˆ—è®ºæ–‡

```
@inproceedings{
  zeng2023glm-130b,
  title={{GLM}-130B: An Open Bilingual Pre-trained Model},
  author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Zhiyuan Liu and Peng Zhang and Yuxiao Dong and Jie Tang},
  booktitle={The Eleventh International Conference on Learning Representations (ICLR)},
  year={2023},
  url={https://openreview.net/forum?id=-Aw0rrrPUF}
}
```
```
@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}
```
